# Ralph Progress Log
---

## Codebase Patterns
- Config loading priority: env vars > .mat-config file > defaults
- Use dataclasses for Settings and similar config structures
- Type annotations required (strict mypy enabled in pyproject.toml)
- Use `ChatCompletionMessageParam` from `openai.types.chat` for typed message lists
- OllamaClient exposes both `chat()` (blocking) and `chat_stream()` (generator) methods
- BaseAgent uses dataclass with default_factory for mutable fields (client, history)
- Token estimation: ~4 chars per token (conservative for context window management)

---

## 2026-01-19 - US-001
- What was implemented: Configuration management module
- Files changed:
  - config/__init__.py (new)
  - config/settings.py (new)
  - pyproject.toml (new)
- **Learnings for future iterations:**
  - Settings class supports loading from env vars, .mat-config file, or defaults
  - Use `get_settings()` for global singleton access
  - Use `reload_settings()` to force reload of configuration
  - Environment variables: MAT_OLLAMA_URL, MAT_MODEL, MAT_PROJECT_DIR, MAT_VERBOSE, MAT_MAX_RETRIES, MAT_TIMEOUT
---

## 2026-01-19 - US-002
- What was implemented: Ollama integration layer with OpenAI-compatible API
- Files changed:
  - llm/__init__.py (new)
  - llm/client.py (new)
- **Learnings for future iterations:**
  - OllamaClient uses OpenAI SDK with `base_url="{ollama_url}/v1"` and dummy api_key="ollama"
  - Custom exceptions: `OllamaConnectionError`, `OllamaModelNotFoundError`, `OllamaResponseError`
  - Exponential backoff retry: `time.sleep(2**attempt)` for transient failures
  - Streaming uses generator pattern with `chat_stream()` yielding string chunks
  - `_list_available_models()` uses OpenAI `models.list()` endpoint for error messages
  - Conversation history format: `[{"role": "user"|"assistant", "content": "..."}]`
---

## 2026-01-19 - US-003
- What was implemented: Base agent class for all MAT agents
- Files changed:
  - agents/__init__.py (new)
  - agents/base.py (new)
- **Learnings for future iterations:**
  - BaseAgent is a dataclass with fields: name, role, system_prompt, client, conversation_history, max_context_tokens, max_retries
  - Uses Message dataclass for history entries: `Message(role="user"|"assistant", content="...")`
  - `chat(message)` handles history management, context truncation, and retries automatically
  - Context window management: reserves 1024 tokens for system prompt + new message, truncates oldest messages first
  - Token estimation uses ~4 chars/token heuristic
  - On non-recoverable errors (connection, model not found), user message is removed from history
  - Use `clear_history()` to reset conversation, `get_history_summary()` for stats
---
