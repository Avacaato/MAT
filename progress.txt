# Ralph Progress Log
---

## Codebase Patterns
- Config loading priority: env vars > .mat-config file > defaults
- Use dataclasses for Settings and similar config structures
- Type annotations required (strict mypy enabled in pyproject.toml)
- Use `ChatCompletionMessageParam` from `openai.types.chat` for typed message lists
- OllamaClient exposes both `chat()` (blocking) and `chat_stream()` (generator) methods
- BaseAgent uses dataclass with default_factory for mutable fields (client, history)
- Token estimation: ~4 chars per token (conservative for context window management)
- File operations: use `utils.file_ops` (sandboxed to project_dir), not raw file I/O
- FileOpsError raised for security violations (paths outside project dir)
- Logging: use `utils.logger` functions, not raw print() or logging module directly
- `log_agent_action()` and `log_agent_decision()` for consistent agent output formatting
- `StoryProgress` with context manager for build progress bars
- Specialized agents inherit from BaseAgent, add domain-specific state as dataclass fields

---

## 2026-01-19 - US-001
- What was implemented: Configuration management module
- Files changed:
  - config/__init__.py (new)
  - config/settings.py (new)
  - pyproject.toml (new)
- **Learnings for future iterations:**
  - Settings class supports loading from env vars, .mat-config file, or defaults
  - Use `get_settings()` for global singleton access
  - Use `reload_settings()` to force reload of configuration
  - Environment variables: MAT_OLLAMA_URL, MAT_MODEL, MAT_PROJECT_DIR, MAT_VERBOSE, MAT_MAX_RETRIES, MAT_TIMEOUT
---

## 2026-01-19 - US-002
- What was implemented: Ollama integration layer with OpenAI-compatible API
- Files changed:
  - llm/__init__.py (new)
  - llm/client.py (new)
- **Learnings for future iterations:**
  - OllamaClient uses OpenAI SDK with `base_url="{ollama_url}/v1"` and dummy api_key="ollama"
  - Custom exceptions: `OllamaConnectionError`, `OllamaModelNotFoundError`, `OllamaResponseError`
  - Exponential backoff retry: `time.sleep(2**attempt)` for transient failures
  - Streaming uses generator pattern with `chat_stream()` yielding string chunks
  - `_list_available_models()` uses OpenAI `models.list()` endpoint for error messages
  - Conversation history format: `[{"role": "user"|"assistant", "content": "..."}]`
---

## 2026-01-19 - US-003
- What was implemented: Base agent class for all MAT agents
- Files changed:
  - agents/__init__.py (new)
  - agents/base.py (new)
- **Learnings for future iterations:**
  - BaseAgent is a dataclass with fields: name, role, system_prompt, client, conversation_history, max_context_tokens, max_retries
  - Uses Message dataclass for history entries: `Message(role="user"|"assistant", content="...")`
  - `chat(message)` handles history management, context truncation, and retries automatically
  - Context window management: reserves 1024 tokens for system prompt + new message, truncates oldest messages first
  - Token estimation uses ~4 chars/token heuristic
  - On non-recoverable errors (connection, model not found), user message is removed from history
  - Use `clear_history()` to reset conversation, `get_history_summary()` for stats
---

## 2026-01-19 - US-004
- What was implemented: File operations module with sandboxed read/write/list capabilities
- Files changed:
  - utils/__init__.py (new)
  - utils/file_ops.py (new)
- **Learnings for future iterations:**
  - Use `read_file(path)`, `write_file(path, content)`, `list_files(directory, pattern)` from utils.file_ops
  - All paths are sandboxed to `get_settings().project_dir` - paths outside are rejected with `FileOpsError`
  - `read_file()` returns empty string for non-existent files (with warning log)
  - Binary files (by extension) are skipped with warning, max file size is 1MB
  - `write_file()` auto-creates parent directories
  - `list_files()` supports glob patterns including recursive `**/*.py`
  - Optional `project_dir` parameter overrides global settings for testing
---

## 2026-01-19 - US-005
- What was implemented: Progress tracking and logging utilities
- Files changed:
  - utils/logger.py (new)
  - utils/__init__.py (updated - added logger exports)
- **Learnings for future iterations:**
  - Use `setup_logging(verbose=True/False)` to configure logging once at startup
  - `get_logger()` returns the MAT logger, auto-initializes if needed
  - `log_agent_action(agent_name, action, details)` for uniform agent output
  - `log_agent_decision(agent_name, decision, reasoning)` logs decisions with optional debug reasoning
  - `log_verbose(message, **kwargs)` only logs in verbose mode (DEBUG level)
  - `StoryProgress` is a context manager: `with create_progress_tracker(total, completed) as progress:`
  - Use `progress.begin_story()`, `progress.complete_story()`, `progress.fail_story()` to update
  - `log_build_start()` and `log_build_complete()` bracket the build with formatted output
  - Logs always go to both console (with rich formatting) and `build.log` file (plain text)
  - File logging is DEBUG level (verbose), console respects verbose setting
---

## 2026-01-19 - US-006
- What was implemented: Product Manager agent for discovery interviews
- Files changed:
  - agents/pm.py (new)
  - agents/__init__.py (updated - added ProductManagerAgent exports)
- **Learnings for future iterations:**
  - ProductManagerAgent inherits from BaseAgent and adds interview-specific logic
  - Uses DiscoveryPhase enum to track interview state: PROBLEM → USERS → FEATURES → SUCCESS → SCOPE → SUMMARY → COMPLETE
  - DiscoveryFindings dataclass stores responses for each phase
  - `start_interview()` resets state and returns opening message with first question
  - `process_response(user_response)` handles the interview flow:
    - Stores user response for current phase
    - Uses LLM to check if clarification is needed
    - Advances to next phase or asks follow-up question
  - `generate_summary()` uses LLM to create structured summary from findings
  - `get_findings()` returns dict of collected responses
  - `is_interview_complete()` checks if all phases done
  - Discovery questions stored in DISCOVERY_QUESTIONS dict, not hardcoded in methods
---

## 2026-01-19 - US-007
- What was implemented: Architect agent for technical solution design
- Files changed:
  - agents/architect.py (new)
  - agents/__init__.py (updated - added ArchitectAgent exports)
- **Learnings for future iterations:**
  - ArchitectAgent inherits from BaseAgent, adds ArchitectureDocument for state
  - Data classes: TechStackProposal, ComponentSpec, DataModel, ArchitectureDocument
  - `propose_tech_stack(requirements)` - parses LLM output into TechStackProposal
  - `identify_components(requirements)` - extracts components from LLM response (separated by ---)
  - `design_data_models(requirements)` - extracts models with fields and relationships
  - `design_api(requirements)` - extracts API endpoints (METHOD /path format)
  - `create_full_architecture(requirements)` - orchestrates full design process
  - `document_decision(decision, rationale)` - manually record architecture decisions
  - `get_architecture_markdown()` - converts current architecture to markdown
  - LLM output parsing: lines start with KEY:, blocks separated by ---
  - All dataclasses have `to_dict()` and relevant `to_markdown()` methods
---

## 2026-01-19 - US-008
- What was implemented: Developer agent for implementing user stories
- Files changed:
  - agents/developer.py (new)
  - agents/__init__.py (updated - added DeveloperAgent exports)
- **Learnings for future iterations:**
  - DeveloperAgent inherits from BaseAgent, adds current_story and context_files state
  - UserStory dataclass with `from_dict()` to load from prd.json format and `to_prompt()` for LLM
  - CodeFile dataclass represents generated code: path and content
  - ImplementationPlan dataclass: files_to_create, files_to_modify, approach
  - `set_story(story)` - sets story and clears history/context for fresh implementation
  - `read_context_file(path)` and `read_context_files(paths)` - load existing code for context
  - `find_related_files(pattern)` - uses `list_files()` to discover project files
  - `analyze_story()` - uses LLM to create ImplementationPlan with files and approach
  - `generate_code(file_path)` - generates new file content with story + context
  - `modify_code(file_path, existing)` - modifies existing file to satisfy story
  - `_extract_code(response)` - strips markdown code blocks from LLM output
  - `write_code_file(code_file)` - uses `write_file()` to persist generated code
  - `implement_story(story)` - orchestrates full implementation: plan → read context → generate → write
  - Agent prompts emphasize following existing patterns and keeping changes focused
---

## 2026-01-19 - US-009
- What was implemented: UX Designer agent for user interface design
- Files changed:
  - agents/ux.py (new)
  - agents/__init__.py (updated - added UXDesignerAgent exports, aliased ComponentSpec to avoid collision)
- **Learnings for future iterations:**
  - UXDesignerAgent inherits from BaseAgent, adds UXDocument for state
  - Data classes: ComponentSpec, UserFlowStep, UserFlow, InteractionSpec, UXDocument
  - Note: Both architect.py and ux.py have `ComponentSpec` - imported as `ArchComponentSpec` and `UXComponentSpec` in __init__.py
  - `create_component_spec(name, requirements)` - parses component specs with props, accessibility, states
  - `define_user_flow(name, requirements)` - parses flows with numbered steps (ACTION | RESULT format)
  - `define_interactions(context)` - parses interaction specs (TRIGGER, ACTION, FEEDBACK, A11Y)
  - `analyze_accessibility(requirements)` - generates WCAG 2.1 AA recommendations
  - `add_accessibility_note(note)` - manually add a11y notes
  - `create_full_ux_design(requirements)` - orchestrates overview, accessibility, flows, interactions
  - `get_ux_markdown()` - converts current UX document to markdown
  - `reset_ux_document()` - clears state and history for fresh design
  - System prompt emphasizes accessibility (WCAG 2.1 AA), keyboard navigation, screen readers
---
