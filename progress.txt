# Ralph Progress Log
---

## Codebase Patterns
- Config loading priority: env vars > .mat-config file > defaults
- Use dataclasses for Settings and similar config structures
- Type annotations required (strict mypy enabled in pyproject.toml)
- Use `ChatCompletionMessageParam` from `openai.types.chat` for typed message lists
- OllamaClient exposes both `chat()` (blocking) and `chat_stream()` (generator) methods
- BaseAgent uses dataclass with default_factory for mutable fields (client, history)
- Token estimation: ~4 chars per token (conservative for context window management)
- File operations: use `utils.file_ops` (sandboxed to project_dir), not raw file I/O
- FileOpsError raised for security violations (paths outside project dir)
- Logging: use `utils.logger` functions, not raw print() or logging module directly
- `log_agent_action()` and `log_agent_decision()` for consistent agent output formatting
- `StoryProgress` with context manager for build progress bars
- Specialized agents inherit from BaseAgent, add domain-specific state as dataclass fields

---

## 2026-01-19 - US-001
- What was implemented: Configuration management module
- Files changed:
  - config/__init__.py (new)
  - config/settings.py (new)
  - pyproject.toml (new)
- **Learnings for future iterations:**
  - Settings class supports loading from env vars, .mat-config file, or defaults
  - Use `get_settings()` for global singleton access
  - Use `reload_settings()` to force reload of configuration
  - Environment variables: MAT_OLLAMA_URL, MAT_MODEL, MAT_PROJECT_DIR, MAT_VERBOSE, MAT_MAX_RETRIES, MAT_TIMEOUT
---

## 2026-01-19 - US-002
- What was implemented: Ollama integration layer with OpenAI-compatible API
- Files changed:
  - llm/__init__.py (new)
  - llm/client.py (new)
- **Learnings for future iterations:**
  - OllamaClient uses OpenAI SDK with `base_url="{ollama_url}/v1"` and dummy api_key="ollama"
  - Custom exceptions: `OllamaConnectionError`, `OllamaModelNotFoundError`, `OllamaResponseError`
  - Exponential backoff retry: `time.sleep(2**attempt)` for transient failures
  - Streaming uses generator pattern with `chat_stream()` yielding string chunks
  - `_list_available_models()` uses OpenAI `models.list()` endpoint for error messages
  - Conversation history format: `[{"role": "user"|"assistant", "content": "..."}]`
---

## 2026-01-19 - US-003
- What was implemented: Base agent class for all MAT agents
- Files changed:
  - agents/__init__.py (new)
  - agents/base.py (new)
- **Learnings for future iterations:**
  - BaseAgent is a dataclass with fields: name, role, system_prompt, client, conversation_history, max_context_tokens, max_retries
  - Uses Message dataclass for history entries: `Message(role="user"|"assistant", content="...")`
  - `chat(message)` handles history management, context truncation, and retries automatically
  - Context window management: reserves 1024 tokens for system prompt + new message, truncates oldest messages first
  - Token estimation uses ~4 chars/token heuristic
  - On non-recoverable errors (connection, model not found), user message is removed from history
  - Use `clear_history()` to reset conversation, `get_history_summary()` for stats
---

## 2026-01-19 - US-004
- What was implemented: File operations module with sandboxed read/write/list capabilities
- Files changed:
  - utils/__init__.py (new)
  - utils/file_ops.py (new)
- **Learnings for future iterations:**
  - Use `read_file(path)`, `write_file(path, content)`, `list_files(directory, pattern)` from utils.file_ops
  - All paths are sandboxed to `get_settings().project_dir` - paths outside are rejected with `FileOpsError`
  - `read_file()` returns empty string for non-existent files (with warning log)
  - Binary files (by extension) are skipped with warning, max file size is 1MB
  - `write_file()` auto-creates parent directories
  - `list_files()` supports glob patterns including recursive `**/*.py`
  - Optional `project_dir` parameter overrides global settings for testing
---

## 2026-01-19 - US-005
- What was implemented: Progress tracking and logging utilities
- Files changed:
  - utils/logger.py (new)
  - utils/__init__.py (updated - added logger exports)
- **Learnings for future iterations:**
  - Use `setup_logging(verbose=True/False)` to configure logging once at startup
  - `get_logger()` returns the MAT logger, auto-initializes if needed
  - `log_agent_action(agent_name, action, details)` for uniform agent output
  - `log_agent_decision(agent_name, decision, reasoning)` logs decisions with optional debug reasoning
  - `log_verbose(message, **kwargs)` only logs in verbose mode (DEBUG level)
  - `StoryProgress` is a context manager: `with create_progress_tracker(total, completed) as progress:`
  - Use `progress.begin_story()`, `progress.complete_story()`, `progress.fail_story()` to update
  - `log_build_start()` and `log_build_complete()` bracket the build with formatted output
  - Logs always go to both console (with rich formatting) and `build.log` file (plain text)
  - File logging is DEBUG level (verbose), console respects verbose setting
---

## 2026-01-19 - US-006
- What was implemented: Product Manager agent for discovery interviews
- Files changed:
  - agents/pm.py (new)
  - agents/__init__.py (updated - added ProductManagerAgent exports)
- **Learnings for future iterations:**
  - ProductManagerAgent inherits from BaseAgent and adds interview-specific logic
  - Uses DiscoveryPhase enum to track interview state: PROBLEM → USERS → FEATURES → SUCCESS → SCOPE → SUMMARY → COMPLETE
  - DiscoveryFindings dataclass stores responses for each phase
  - `start_interview()` resets state and returns opening message with first question
  - `process_response(user_response)` handles the interview flow:
    - Stores user response for current phase
    - Uses LLM to check if clarification is needed
    - Advances to next phase or asks follow-up question
  - `generate_summary()` uses LLM to create structured summary from findings
  - `get_findings()` returns dict of collected responses
  - `is_interview_complete()` checks if all phases done
  - Discovery questions stored in DISCOVERY_QUESTIONS dict, not hardcoded in methods
---
